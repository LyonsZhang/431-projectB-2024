{
  "hash": "ccd56563a75dc42f37c8c9246c5550ae",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Required Study 2 Analyses\"\nauthor: \"431 Staff\"\ndate: last-modified\n---\n\n\n\n\nOnce you have identified an acceptable data set, you will produce a report that demonstrates that you have accomplished the following:\n\n1. Identify a quantitative outcome. \n    - For purposes of this project, we will require your quantitative outcome to contain more than 15 unique values.\n2. Identify a key predictor (which may be either quantitative or categorical.)\n    - If the key predictor is categorical, it must have 3-6 categories, and each category must contain at least 30 observations.\n3. Identify 3-8 other predictors of your outcome (demonstrating that either your key predictor or at least one of the \"other\" predictors is multi-categorical with 3-6 categories.)\n4. Define a research question related to how effectively your key predictor predicts your quantitative outcome, while (possibly) adjusting for the other predictors.\n5. Steps 1-3 will yield a set of 6-11 variables (an outcome, a key predictor, 3-8 other predictors, and a subject identifier). Use those selections to create your analytic data set.\n    - You must have between (500 and 7,500 observations if you're using NHANES; 250 and 10,000 observations if not using NHANES) with complete data on all 6-11 variables included in your Study 2 analytic tibble. No other variables should be included in your Study 2 analytic tibble.\n6. Clean the data in R, and this includes the creation of appropriately labeled (and if necessary, collapsed) factors for all categorical variables, and the investigation and decision-making regarding missing values, numbers of unique values and impossible values.\n7. Complete any imputation required to deal with missing data. Use single (simple) imputation with the `mice` package to create your imputed data. Do not impute your outcome or key predictor - you should filter to complete cases on those two variables.\n8. Use appropriate tools to provide useful numerical summaries for all data you will study, after all cleaning, so that these results describe the exact variables you will be modeling in the remaining work. Provide a clear note describing how much imputation you did.\n9. Partition the clean data into a model development (also called a model training) sample (60-80% of the data) and a model testing (also called a model validation) sample (the remaining 20-40%) using the approach recommended in the data .\n    - Suppose you had a tibble called **original_data** which identified its subjects with a **subjID** variable, and you wanted to place 75% of your data for development into **training_sample** and the rest into **test_sample**. You could do that with the following code...\n    \n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\nset.seed(431) # pick a different seed than this\n\ntraining_sample <- original_data |> slice_sample(prop = 0.75)\n\ntest_sample <- \n    anti_join(original_data, training_sample, by = \"subjID\")\n```\n:::\n\n    \n10. Provide appropriate, well-labeled visualizations of your outcome, and investigate potential transformations of that outcome for the purpose of fitting regression models in a useful way. Whatever transformation (including no transformation at all) should be used for the steps that follow.\n11. Produce two competitive models fit using least squares (rather than a Bayesian approach) for predicting your outcome using your clean data that provide evidence regarding your research question.\n    - One of these models should be the full model with all candidate predictors included.\n    - Your other model should be a well-motivated subset of your full model, that at least includes the key predictor. The naive strategy of using as your subset the key predictor alone is 100% appropriate.\n12. Assess the performance of your two models (full model or subset) and come to a conclusion about which is better. This assessment should includes both in-sample (predictive performance and adherence to assumptions) and holdout sample (predictive quality) assessments. Be sure to attend to back-transformation properly should that be necessary, in evaluating the quality of predictions.\n13. Use the results of the model you chose to answer your research question, and then describe the limitations of this study and next steps you would like to pursue.\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}